# ContextMine Environment Configuration

# API settings
DEBUG=true
API_HOST=0.0.0.0
API_PORT=8000

# Database
# For local development with Docker Compose:
DATABASE_URL=postgresql+asyncpg://contextmine:contextmine@localhost:5432/contextmine
# For running inside Docker network:
# DATABASE_URL=postgresql+asyncpg://contextmine:contextmine@postgres:5432/contextmine

# GitHub OAuth (required for authentication)
# Create a GitHub OAuth App at: https://github.com/settings/developers
# Set Authorization callback URL to: http://localhost:8000/api/auth/callback
# (Both admin UI and MCP clients use this single callback URL)
GITHUB_CLIENT_ID=your_github_client_id
GITHUB_CLIENT_SECRET=your_github_client_secret

# Public URL for OAuth callbacks (must match GitHub OAuth App settings)
PUBLIC_BASE_URL=http://localhost:8000

# Security keys (generate secure random values for production!)
# Generate with: python -c "import secrets; print(secrets.token_urlsafe(32))"
SESSION_SECRET=dev-session-secret-change-in-production
TOKEN_ENCRYPTION_KEY=dev-encryption-key-change-in-production

# MCP Security
# Comma-separated list of allowed origins for MCP endpoint
# Empty = allow all origins (dev mode only!)
# Example: MCP_ALLOWED_ORIGINS=https://claude.ai,https://your-app.com
MCP_ALLOWED_ORIGINS=

# MCP OAuth base URL (for OAuth callbacks)
# Must match where your server is accessible
MCP_OAUTH_BASE_URL=http://localhost:8000

# LLM Providers (at least one required for context assembly)
# OPENAI_API_KEY=sk-...
# ANTHROPIC_API_KEY=sk-ant-...
# GEMINI_API_KEY=...

# Default models
# DEFAULT_EMBEDDING_MODEL=openai:text-embedding-3-small
# DEFAULT_LLM_PROVIDER=openai
# DEFAULT_LLM_MODEL=gpt-4o-mini

# Research Agent (advanced code investigation)
# Artifact storage: 'memory' or 'file'
ARTIFACT_STORE=memory
# Directory for file-backed artifact store
ARTIFACT_DIR=.mcp_artifacts
# Artifact TTL in minutes
ARTIFACT_TTL_MINUTES=60
# Maximum number of research runs to keep
ARTIFACT_MAX_RUNS=100
# Model for research agent reasoning
RESEARCH_MODEL=claude-sonnet-4-5-20250929
# Max tokens per research agent LLM call
RESEARCH_MAX_TOKENS=4096
# Default maximum steps for research agent
RESEARCH_BUDGET_STEPS=10

# SCIP Polyglot Indexing
# Comma-separated list of languages to index
SCIP_LANGUAGES=python,typescript,javascript,java,php
# Dependency installation mode: auto, always, never
SCIP_INSTALL_DEPS_MODE=auto
# Indexing timeouts per language (seconds)
SCIP_TIMEOUT_PYTHON=300
SCIP_TIMEOUT_TYPESCRIPT=600
SCIP_TIMEOUT_JAVA=900
SCIP_TIMEOUT_PHP=300
# Node.js memory limit for TS/JS/Python indexers (MB)
SCIP_NODE_MEMORY_MB=4096
# Continue indexing if individual projects fail
SCIP_BEST_EFFORT=true

# OpenTelemetry (optional - disabled by default)
# Enable distributed tracing, metrics, and log correlation
OTEL_ENABLED=false
# Service name (will be suffixed with -api, -worker)
OTEL_SERVICE_NAME=contextmine
# OTLP exporter endpoint (gRPC or HTTP)
# Examples: http://localhost:4317 (gRPC), http://localhost:4318/v1/traces (HTTP)
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
# Protocol: 'grpc' or 'http/protobuf'
OTEL_EXPORTER_OTLP_PROTOCOL=grpc
# Trace sampling: always_on, always_off, traceidratio, parentbased_traceidratio
OTEL_TRACES_SAMPLER=parentbased_traceidratio
# Sampling ratio (0.0-1.0, e.g., 0.1 for 10% sampling)
OTEL_TRACES_SAMPLER_ARG=1.0
# Minimum log level to export: DEBUG, INFO, WARNING, ERROR
OTEL_LOG_LEVEL=INFO
