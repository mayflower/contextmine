# ContextMine Environment Configuration

# API settings
DEBUG=true
API_HOST=0.0.0.0
API_PORT=8000

# Database
# For local development with Docker Compose:
DATABASE_URL=postgresql+asyncpg://contextmine:contextmine@localhost:5432/contextmine
# For running inside Docker network:
# DATABASE_URL=postgresql+asyncpg://contextmine:contextmine@postgres:5432/contextmine

# GitHub OAuth (required for authentication)
# Create a GitHub OAuth App at: https://github.com/settings/developers
# Set Authorization callback URL to: http://localhost:8000/api/auth/callback
# (Both admin UI and MCP clients use this single callback URL)
GITHUB_CLIENT_ID=your_github_client_id
GITHUB_CLIENT_SECRET=your_github_client_secret

# Public URL for OAuth callbacks (must match GitHub OAuth App settings)
PUBLIC_BASE_URL=http://localhost:8000

# Security keys (generate secure random values for production!)
# Generate with: python -c "import secrets; print(secrets.token_urlsafe(32))"
SESSION_SECRET=dev-session-secret-change-in-production
TOKEN_ENCRYPTION_KEY=dev-encryption-key-change-in-production

# MCP Security
# Comma-separated list of allowed origins for MCP endpoint
# Empty = allow all origins (dev mode only!)
# Example: MCP_ALLOWED_ORIGINS=https://claude.ai,https://your-app.com
MCP_ALLOWED_ORIGINS=

# MCP OAuth base URL (for OAuth callbacks)
# Must match where your server is accessible
MCP_OAUTH_BASE_URL=http://localhost:8000

# LLM Providers (at least one required for context assembly)
# OPENAI_API_KEY=sk-...
# ANTHROPIC_API_KEY=sk-ant-...
# GEMINI_API_KEY=...

# Default models
# DEFAULT_EMBEDDING_MODEL=openai:text-embedding-3-small
# DEFAULT_LLM_PROVIDER=openai
# DEFAULT_LLM_MODEL=gpt-4o-mini

# Research Agent (advanced code investigation)
# Artifact storage: 'memory' or 'file'
ARTIFACT_STORE=memory
# Directory for file-backed artifact store
ARTIFACT_DIR=.mcp_artifacts
# Artifact TTL in minutes
ARTIFACT_TTL_MINUTES=60
# Maximum number of research runs to keep
ARTIFACT_MAX_RUNS=100
# Model for research agent reasoning
RESEARCH_MODEL=claude-sonnet-4-5-20250929
# Max tokens per research agent LLM call
RESEARCH_MAX_TOKENS=4096
# Default maximum steps for research agent
RESEARCH_BUDGET_STEPS=10
